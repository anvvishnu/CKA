LFS 258
=====****====

Notes in <+++ notes +++> means this is not in the output of the command but added in notepad++ for reference.

https://kubernetes.io/docs/concepts/cluster-administration/networking/

Networking for k8s

In the google could k8s cluster that we created with three VM we used:

Weave Net from Weaveworks
-------------------------
Weave Net is a resilient and simple to use network for Kubernetes and its hosted applications. Weave Net runs as a CNI plug-in or stand-alone. In either version, it doesnâ€™t require any configuration or extra code to run, and in both cases, the network provides one IP address per pod - as is standard for Kubernetes.

CNI: Container Network Interface
https://github.com/containernetworking/cni

CNI plugin can be used to configure the network of a pod and provide a single IP per pod
CNI does not help in pod-to-pod communication across nodes

Weave is a software defined overlay solution that enables this.

NAT - Network Address Translation

Mesos: http://mesos.apache.org/documentation/latest/architecture/

============
Lab 4.1
============

Deployment :: hog, pod:: stress

root@instance-1:~/test/CKA/LFS258# kubectl create deployment hog --image vish/stress
deployment.apps/hog created
root@instance-1:~/test/CKA/LFS258#


root@instance-1:~/test/CKA/LFS258# kubectl create deployment hog --image vish/stress
deployment.apps/hog created
root@instance-1:~/test/CKA/LFS258# kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
hello-deploy   3/3     3            3           5h28m
hog            1/1     1            1           2m51s
root@instance-1:~/test/CKA/LFS258# kubectl describe deployment hog
Name:                   hog
Namespace:              default
CreationTimestamp:      Mon, 11 Mar 2019 02:15:15 +0000
Labels:                 app=hog
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=hog
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=hog
  Containers:
   stress:
    Image:        vish/stress
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   hog-6848f56478 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  3m7s  deployment-controller  Scaled up replica set hog-6848f56478 to 1
root@instance-1:~/test/CKA/LFS258# kubectl get deployment hog -o yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2019-03-11T02:15:15Z"
  generation: 1
  labels:
    app: hog
  name: hog
  namespace: default
  resourceVersion: "102417"
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/hog
  uid: 81ff98e9-43a3-11e9-8f7d-42010a8e0002
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: hog
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hog
    spec:
      containers:
      - image: vish/stress
        imagePullPolicy: Always
        name: stress
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2019-03-11T02:15:18Z"
    lastUpdateTime: "2019-03-11T02:15:18Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2019-03-11T02:15:15Z"
    lastUpdateTime: "2019-03-11T02:15:18Z"
    message: ReplicaSet "hog-6848f56478" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
root@instance-1:~/test/CKA/LFS258# kubectl get deployment hog \
> --export -o yaml > hog.yml
root@instance-1:~/test/CKA/LFS258# ls -l
total 4
-rw-r--r-- 1 root root 971 Mar 11 02:19 hog.yml

==Make changes in hog.yml as mentioned in the lab pdf file==

root@instance-1:~/test/CKA/LFS258# kubectl replace -f hog.yml
deployment.extensions/hog replaced
root@instance-1:~/test/CKA/LFS258# kubectl get deployment hog -o yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "2"
  creationTimestamp: "2019-03-11T02:15:15Z"
  generation: 2
  labels:
    app: hog
  name: hog
  namespace: default
  resourceVersion: "107350"
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/hog
  uid: 81ff98e9-43a3-11e9-8f7d-42010a8e0002
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: hog
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hog
    spec:
      containers:
      - image: vish/stress
        imagePullPolicy: Always
        name: stress
        resources:
          limits:
            memory: 4Gi    <+++++ This one was added in the hog.yml and the deployment was replaced +++++>
          requests:
            memory: 2500Mi   <+++++ This one was added in the hog.yml and the deployment was replaced +++++>
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2019-03-11T02:15:18Z"
    lastUpdateTime: "2019-03-11T02:15:18Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2019-03-11T02:15:15Z"
    lastUpdateTime: "2019-03-11T03:10:40Z"
    message: ReplicaSet "hog-fbf9cf454" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 2
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
root@instance-1:~/test/CKA/LFS258# kubectl get po
NAME                            READY   STATUS    RESTARTS   AGE
hello-deploy-747d4479fb-8vk4f   1/1     Running   0          6h16m
hello-deploy-747d4479fb-n7wzt   1/1     Running   0          6h16m
hello-deploy-747d4479fb-vr5jc   1/1     Running   0          6h16m
hog-fbf9cf454-nrl4x             1/1     Running   0          28s
root@instance-1:~/test/CKA/LFS258# kubectl logs hog-fbf9cf454-nrl4x
I0311 03:10:39.559504       1 main.go:26] Allocating "0" memory, in "4Ki" chunks, with a 1ms sleep between allocations
I0311 03:10:39.559573       1 main.go:29] Allocated "0" memory

=== adding args === (the file has details on the error that we got args need to be aligned to resources)

root@instance-1:~/test/CKA/LFS258# kubectl delete deployment hog
deployment.extensions "hog" deleted
root@instance-1:~/test/CKA/LFS258# vim hog.yml
root@instance-1:~/test/CKA/LFS258# kubectl create -f hog.yml
deployment.extensions/hog created
root@instance-1:~/test/CKA/LFS258# vi hog_new.yml
root@instance-1:~/test/CKA/LFS258# kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
hello-deploy   3/3     3            3           6h34m
hog            1/1     1            1           40s
root@instance-1:~/test/CKA/LFS258#

After the change when we look at instance3 top you can find stress here (you will need to check in both the nodes as we don't know which node the pod will be running, in this case it is running in node2 which is instance3)

total 0
root@instance-3:~# top
top - 03:24:32 up 20:59,  1 user,  load average: 1.49, 0.50, 0.17
Tasks: 127 total,   1 running,  88 sleeping,   0 stopped,   0 zombie
%Cpu(s): 20.6 us, 79.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  7652344 total,  3348264 free,  1422780 used,  2881300 buff/cache
KiB Swap:        0 total,        0 free,        0 used.  5924228 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 7395 root      20   0  958536 954912   3184 S 195.0 12.5   2:37.87 stress
 5285 root      20   0 1350820  93368  57880 S   1.7  1.2  31:59.46 kubelet
 4660 root      20   0 1263972  95944  40952 S   0.7  1.3  12:02.34 dockerd
 5773 root      20   0  137180  30616  23740 S   0.7  0.4   3:03.96 kube-proxy
 4683 root      20   0 1232752  38624  22688 S   0.3  0.5   4:17.27 docker-containe
 6439 root      20   0  365480  19672  15268 S   0.3  0.3   0:20.70 kube-utils
 6539 root      20   0   42240   3792   3184 R   0.3  0.0   0:00.92 top
    1 root      20   0  159980   9304   6700 S   0.0  0.1   0:04.80 systemd
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.01 kthreadd


=== Checking the logs of the pod, incase pod is showing running but memory not allocated as mentioned above ====

root@instance-1:~/test/CKA/LFS258# kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
hello-deploy-747d4479fb-8vk4f   1/1     Running   0          6h37m
hello-deploy-747d4479fb-n7wzt   1/1     Running   0          6h36m
hello-deploy-747d4479fb-vr5jc   1/1     Running   0          6h37m
hog-67f7df994f-7cgtb            1/1     Running   0          8m25s
root@instance-1:~/test/CKA/LFS258# kubectl logs hog-67f7df994f-7cgtb
I0311 03:23:11.786767       1 main.go:26] Allocating "950Mi" memory, in "100Mi" chunks, with a 1s sleep between allocations
I0311 03:23:11.786837       1 main.go:39] Spawning a thread to consume CPU
I0311 03:23:11.786861       1 main.go:39] Spawning a thread to consume CPU
I0311 03:23:24.640055       1 main.go:29] Allocated "950Mi" memory
root@instance-1:~/test/CKA/LFS258#


